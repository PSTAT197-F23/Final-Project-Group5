---
title: "Clustering"
author: 'Aron, Dylan, Kaitlyn, Sanaz, Yoobin'
date: 12/6/2023
---

In this vignette, we will take a deep dive into clustering and explain the methodology and processes in order to give you the tools to perform clustering for machine learning applications. We will delve into K-means and hierarchical clustering, and we will talk about the reasons to use or not to use hierarchical clustering.

Objectives: Learn clustering methods, when and why to use them. Introduce hierarchical clustering and compare the benefits and downsides. Learn visualization methods such as dendrograms.

# Setup

For this activity we will be using the `iris` dataset. This dataset consists of measurements taken from three different species of iris flowers: Versicolor, Setosa, and Virginica.

![](images/iris-flowers.png){fig-align="center" width="441"}

```{r}
# packages:
library(tidyverse)
library(ggplot2)

set.seed(0)

# import data:
data("iris")
iris %>% head(4)
```

The `iris` set is made up of 5 variables and 150 observations, with each observation being an iris flower. 4 of the variables give measurements of sepal length, sepal width, petal length, petal width. The last variable is `Species`, which we will not be using as it would have been the response variable for supervised methods. The other 4 variables will be used as predictors.

Your job is to sort the data into clusters using k-means and hierarchical clustering.

# Clustering

## K-means

### Step 1: PCA

Since we have more than 2 predictors, our first step will be to perform Principal Component Analysis to reduce the dimensionality of the data down to 2 axes.

```{r}
# Calculate the principal components using prcomp() function
pca <- prcomp(iris[, -5], scale = TRUE)

# Extract the scores for the first two principal components
pc1 <- pca$x[, 1]
pc2 <- pca$x[, 2]
#get a summary of the components
summary(pca)
```

We will be creating clusters using these components.

Before that, lets take a look at our data when plotted on the axes given by the components:

```{r}
iris_pca <- data.frame(cbind(pc1, pc2))
colnames(iris_pca)[1:2] <- c("PC1", "PC2")

# Create a scatter plot of the first two principal components
ggplot(iris_pca, aes(x = PC1, y = PC2)) +
  geom_point()
```

Visual observation is one way of choosing the number of clusters to use in k-means clustering. Based on this plot, we would likely choose 2 as our number of clusters. However, lets try a more rigorous method before making any decisions.

### Step 2: Elbow Method

K-means clustering requires us to pre-select the number of clusters we will be grouping our data into. This is done using the elbow method, in which we first vary the number of clusters and calculate the within-cluster sum of squares. Then, we plot the sum of squares and \# of clusters visually and find where the change in SS starts to level off, selecting that point as our chosen number of clusters.

```{r}
#calculate ss for each # of clusters 
inertia<-c()
for (i in 1:10){
  m<-kmeans(iris_pca, centers = i) # perform clustering
  inertia<-c(inertia,m$tot.withinss)# get within cluster SS for that k 
}

#plot data
ggplot(data = NULL, aes(x = 1:10, y = inertia))+geom_line()+geom_point()+labs(title = "Elbow Plot", x = "n Clusters", y = "Within Cluster SS")+scale_x_continuous(breaks = pretty(1:10, n = 10))
```

As we can see, the change in SS seems to level off after 3. Therefore we will set our number of clusters for k-means as 3.

### Step 3: Clustering

We will now be performing k-means clustering, which is a simple and efficient approach for breaking a dataset into distinct groups. The algorithm involves randomly assigning each observation to an initial cluster, then begins an iterative process of computing the centroid of each cluster and assigning each observation to the cluster with the closest centroid. The goal is to minimize the Euclidean distance, that is:

![](images/euclidean-formula.png){fig-align="center" width="285" height="64"}

where each $C_k$ is a cluster.

The algorithm continues to iterate until the clusters no longer change, or until the maximum iterations are reached.

We will now apply this k-means clustering method to our own dataset in order to group each flower into one of the three species.

```{r}
#cluster
model<-kmeans(iris_pca, centers = 3)

#create dataframe
preds<-cbind(iris_pca, model$cluster)
colnames(preds)[3]<-"Group"

#plot clusters
ggplot(preds, aes(x = PC1, y = PC2, color = as.factor(Group))) +
  geom_point()+labs(title="Predicted Clusters")
```

We have successfully clustered our data using k-means clustering.

## Hierarchical

We will now utilize a different approach to this classification problem: hierarchical clustering.

One of the major differences between k-means and hierarchical clustering is that hierarchical clustering does not require us to specify a number of clusters beforehand. Additionally, we are able to construct **dendrograms**, which are useful tree-based models of the observations.

### Step 1:

```{r}
# cluster:
clusters <- hclust(dist(iris[, 3:4]))

# plot the dendrogram:
plot(clusters)
```

```{r}
# cut the tree at the desired number of clusters (3):
cut <- cutree(clusters, 3)
```

```{r}
table(cut, iris$Species)
```
