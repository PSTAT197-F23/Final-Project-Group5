---
title: "Clustering"
author: 'Aron, Dylan, Kaitlyn, Sanaz, Yoobin'
date: 12/6/2023
---

In this vignette, we will take a deep dive into clustering and explain the methodology and processes in order to give you the tools to perform clustering for machine learning applications. We will delve into K-means and hierarchical clustering, and we will talk about the reasons to use or not to use hierarchical clustering.

Objectives: Learn clustering methods, when and why to use them. Introduce hierarchical clustering and compare the benefits and downsides. Learn visualization methods such as dendrograms.

# Setup

For this activity we will be using the `iris` dataset. This dataset consists of measurements taken from three different species of iris flowers: Versicolor, Setosa, and Virginica.

![](images/iris-flowers.png){fig-align="center" width="441"}

```{r}
# packages:
library(tidyverse)
library(ggplot2)

set.seed(0)

# import data:
data("iris")
iris %>% head(4)
```

The `iris` set is made up of 5 variables and 150 observations, with each observation being an iris flower. 4 of the variables give measurements of sepal length, sepal width, petal length, petal width. The last variable is `Species`, which we will not be using as it would have been the response variable for supervised methods. The other 4 variables will be used as predictors.

Your job is to sort the data into clusters using k-means and hierarchical clustering.

# Clustering

## K-means

### Step 1: PCA

Since we have more than 2 predictors, our first step will be to perform Principal Component Analysis to reduce the dimensionality of the data down to 2 axes. Clustering is a unsupervised method, so to make our data compatible with this process we will extract the outcome variable to yield feasible results.

```{r}
# Calculate the principal components using prcomp() function
pca <- prcomp(iris[, -5], scale = TRUE)

# Extract the scores for the first two principal components
pc1 <- pca$x[, 1]
pc2 <- pca$x[, 2]
#get a summary of the components
summary(pca)
```

We will be creating clusters using these components.

Before that, lets take a look at our data when plotted on the axes given by the components:

```{r}
iris_pca <- data.frame(cbind(pc1, pc2))
colnames(iris_pca)[1:2] <- c("PC1", "PC2")

# Create a scatter plot of the first two principal components
ggplot(iris_pca, aes(x = PC1, y = PC2)) +
  geom_point()
```

Visual observation is one way of choosing the number of clusters to use in k-means clustering. Based on this plot, we would likely choose 2 as our number of clusters. However, lets try a more rigorous method before making any decisions.

### Step 2: Elbow Method

K-means clustering requires us to pre-select the number of clusters we will be grouping our data into. This is done using the elbow method, in which we first vary the number of clusters and calculate the within-cluster sum of squares. Then, we plot the sum of squares and \# of clusters visually and find where the change in SS starts to level off, selecting that point as our chosen number of clusters.

```{r}
#calculate ss for each # of clusters 
inertia<-c()
for (i in 1:10){
  m<-kmeans(iris_pca, centers = i) # perform clustering
  inertia<-c(inertia,m$tot.withinss)# get within cluster SS for that k 
}

#plot data
ggplot(data = NULL, aes(x = 1:10, y = inertia))+geom_line()+geom_point()+labs(title = "Elbow Plot", x = "n Clusters", y = "Within Cluster SS")+scale_x_continuous(breaks = pretty(1:10, n = 10))
```

As we can see, the change in SS seems to level off after 3. Therefore we will set our number of clusters for k-means as 3.

### Step 3: Clustering

We will now be performing k-means clustering, which is a simple and efficient approach for breaking a dataset into distinct groups. The algorithm involves randomly assigning each observation to an initial cluster, then begins an iterative process of computing the centroid of each cluster and assigning each observation to the cluster with the closest centroid. The goal is to minimize the Euclidean distance, that is:

![](images/euclidean-formula.png){fig-align="center" width="285" height="64"}

where each $C_k$ is a cluster.

The algorithm continues to iterate until the clusters no longer change, or until the maximum iterations are reached.

We will now apply this k-means clustering method to our own dataset in order to group each flower into one of the three species.

```{r}
#cluster
model<-kmeans(iris_pca, centers = 3)

#create dataframe
preds<-cbind(iris_pca, model$cluster)
colnames(preds)[3]<-"Group"

#plot clusters
ggplot(preds, aes(x = PC1, y = PC2, color = as.factor(Group))) +
  geom_point()+labs(title="Predicted Clusters")
```

We have successfully clustered our data using k-means clustering.

## Hierarchical

Unlike k-means clustering there is no pre-specified number of clusters to work with. There are two different ways to go about hierarchical clustering: agglomerative and divisive. Agglomerative hierarchical clustering where we follow more of a bottom up method. Each observation will be its own cluster all the way down until all the clusters are merged into one at the end. For divisive clustering it is the opposite fashion where there all the data is in one cluster at the top and it splits down into clusters that contain variables similar to eachother but different than the other clusters created around it(top-down). Another benefit to using hierarchical clustering is that you can choose where to cut off the clusters.

Algorithm/ theory: We first want to create a distance matrix with the euclidean distance function. In finding similarities in clustering the question is always how much space between points is a good threshold, this will usually be determined by the shape of the cluster. Some methods for getting the threshold are: single linkage, complete linkage,centroid linkage,average linkage, ward linkage.

### Dendrograms:

We will perform hierarchical clustering with the default linkage method, which is `complete`.

```{r}
# hierarchical clustering using default method:
clusters_complete <- hclust(dist(iris[, 3:4]), method = 'complete')

# plot the dendrogram:
plot(clusters_complete, main = 'Iris Cluster Dendrogram (Complete linkage)')
abline(h = 3, col = 'red', lty = 'dashed')
```

Agglomerative hierarchical clustering works from the bottom up, with each leaf at the bottom of the dendrogram representing an observation. Working up the tree, leaves that are similar to one another begin to fuse together to create branches. These branches fuse with similar branches, and this keeps continuing up the tree until all observations/branches are fused. Therefore, when we look at the dendrogram, we can tell how similar or different two observations are based off the height at which they fused together --- fusing at the bottom of the tree indicates similarity, while fusing higher up shows more dissimilarity.

Recall that hierarchical clustering enables us to work with any number of clusters based off of a single dendrogram. In order to identify clusters, we can draw horizontal lines across the plot and observe the distinct branches that are made. This height controls the number of clusters we work with, which makes it similar to the value of K in k-means clustering. For example, in the figure above, we added a line at `height = 3`, which created 3 different clusters. However, if we were to cut at `height = 2`, we would end up with 4 groups.

### Tree cutting:

For our specific problem, a good choice would be to cut the tree in order to make 3 clusters, as indicated by our dendrogram as well as our prior knowledge of the number of iris species. We will now move forward with cutting the tree and assessing the clusters that result.

```{r}
# cut the tree at the desired number of clusters (3):
cut_complete <- cutree(clusters_complete, 3)

# observe results in table form:
table(cut_complete, iris$Species)
```

The above table shows that all 50 Setosa flowers were classified by cluster 1 and all 50 Virginica flowers were classified by cluster 2, but the algorithm struggled with Versicolor.

For the sake of exploration, we can also see what would have happened if we decided to cut the tree to make 4 clusters:

```{r}
# cut tree for 4 clusters:
cut_2 <- cutree(clusters, 4)

table(cut_2, iris$Species)
```

### Linkage methods:

Before we try to improve the results of our classification, let us first dive into the workings behind the algorithm to see what is happening behind the scenes. The hierarchical clustering algorithm is an iterative process that starts at the bottom of the dendogram, with each of the n observation being its own cluster. The algorithm then merges the two most similar clusters, reducing the total count to n - 1 clusters. This process continues over and over, fusing the two clusters most similar to each other, until all observations are in a single cluster, which is seen in the top of the dendogram.

The concept of linkage, as mentioned before, comes in to play to define the dissimilarity between two groups of observations. In our computations above, we were using the default `complete` method, which utilizes the maximal intercluster dissimilarity. Other common linkage methods are `single`, `average`, and `centroid`, which are described in the table below.

+:------------:+:------------------------------------------------------------------------------------------------------------------+
| **Complete** | The distance between two clusters is the maximal distance between the farthest points in cluster 1 and 2.         |
|              |                                                                                                                   |
|              | $d_{12} = max(i,j) d(X_i, Y_j)$                                                                                   |
+--------------+-------------------------------------------------------------------------------------------------------------------+
| **Single**   | The distance between two clusters is the minimal distance between the closets points in cluster 1 and 2.          |
|              |                                                                                                                   |
|              | $d_{12} = min(i,j) d(X_i, Y_j)$                                                                                   |
+--------------+-------------------------------------------------------------------------------------------------------------------+
| **Average**  | The distance between two clusters is the average of the distances between all pairs of points in cluster 1 and 2. |
|              |                                                                                                                   |
|              | $d_{12} = \frac1{kl} \sum_{i=1}^k \sum_{i=1}^l d(X_i, Y_j)$                                                       |
+--------------+-------------------------------------------------------------------------------------------------------------------+
| **Centroid** | The distance between two clusters is distance between the two mean vectors of clusters 1 and 2.                   |
|              |                                                                                                                   |
|              | $d_{12} = d(\bar{x}, \bar{y})$                                                                                    |
+--------------+-------------------------------------------------------------------------------------------------------------------+

: Now that we have discussed the different linkage methods, let us try to improve our results by using a different method.

### Final clustering:

```{r}
# hierarchical clustering using average method:
clusters_average <- hclust(dist(iris[, 3:4]), method = 'average')

# plot the dendrogram:
plot(clusters_average, main = 'Iris Cluster Dendrogram (Average linkage)')
abline(h = 1.3, col = 'red', lty = 'dashed')

# cut the tree at the desired number of clusters (3):
cut_average <- cutree(clusters_average, 3)

# observe results in table form:
table(cut_average, iris$Species)
```

As seen in the table, utilizing the `average` linkage method was much more successful in classifying the flowers. We can also visualize these results in the form of a plot:

```{r}
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + 
  geom_point(alpha = 0.4, size = 3) + geom_point(col = cut_average) + 
  scale_color_manual(values = c('black', 'red', 'green'))
```

We have now discussed and gone in detail about two types of clustering: K-means and hierarchical. Note the differences in their respective processes and consider the benefits of using one over the other. K-means requires a specified number of clusters, but is more computationally efficient. Hierarchical is customizable and reproducible, allowing for flexibility. 
